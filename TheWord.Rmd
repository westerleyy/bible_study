---
title: "The Word"
subtitle: "An textual analysis of the King James Version, American Standard Version, and World English Bible."
author: "Wesley Chioh"
date: "April 9, 2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# loading libraries
library(quanteda)
library(tidyverse)
library(topicmodels)
library(ggplot2)
library(lsa)
library(stm)
library(factoextra)
library(text2vec)
library(plotly)
library(DT)
library(caret)
library(doParallel)
library(cluster)
library(kableExtra)

#reading in bible
all_bibles <- read_csv("bibles.csv")

# filtering for gospels
gospels <- all_bibles %>%
  filter(str_detect(Verse, "Matthew|Mark|John|Luke"))
gospels <- gospels %>%
  mutate(Book = str_split(Verse, " ")[[1]][1])

# extracting the name of each book
# extracting the KJV
# paste and collapse each verse grouped by book
all_bibles <- all_bibles %>%
  mutate(Verse = str_replace_all(Verse, c("1 " = "First", "2 " = "Second", "3 " = "Third")))
books <- sapply(1:dim(all_bibles)[1], function(x){
  book = str_split(all_bibles[x,1], " ")[[1]][1]
  book
})
chapters <- sapply(1:dim(all_bibles)[1], function(x){
  chapter = str_split(all_bibles[x,1], ":")[[1]][1]
  chapter
})
all_bibles$Book <- books
all_bibles$Chapter <- chapters
kjv_collapsed <- all_bibles %>%
  select(Book, `King James Bible`) %>%
  group_by(Book) %>%
  summarise(Verse = paste(`King James Bible`, collapse = " "))
kjv_collapsed_vec <- kjv_collapsed$Verse %>%
  unlist()
names(kjv_collapsed_vec) <- unique(books)

# create DFM
# convert to matrix
kjv_books_dfm <- dfm(kjv_collapsed_vec, tolower = T, remove_punct = T, remove_numbers = T, stem = T)
kjv_books_mat <- convert(kjv_books_dfm, to = "matrix")

## ASV
asv_gospels_collapsed <- all_bibles %>%
  filter(Book %in% c("Matthew", "Mark", "Luke", "John")) %>%
  select(Book, `American Standard Version`) %>%
  group_by(Book) %>%
  summarise(Verse = paste(`American Standard Version`, collapse = " "))
asv_collapsed_vec <- asv_gospels_collapsed$Verse %>%
  unlist()
names(asv_collapsed_vec) <- c("Matthew", "Mark", "Luke", "John")

# create DFM
# convert to matrix
asv_books_dfm <- dfm(asv_collapsed_vec, tolower = T, remove_punct = T, remove_numbers = T, stem = T)

## WEB
web_gospels_collapsed <- all_bibles %>%
  filter(Book %in% c("Matthew", "Mark", "Luke", "John")) %>%
  select(Book, `World English Bible`) %>%
  group_by(Book) %>%
  summarise(Verse = paste(`World English Bible`, collapse = " "))
web_collapsed_vec <- web_gospels_collapsed$Verse %>%
  unlist()
names(web_collapsed_vec) <- c("Matthew", "Mark", "Luke", "John")

# create DFM
# convert to matrix
web_books_dfm <- dfm(web_collapsed_vec, tolower = T, remove_punct = T, remove_numbers = T, stem = T)
```

#### Introduction  
  
The New York Times bestseller list changes every week. But on the time scale of centuries and millenia, the longstanding global bestseller has not. It is most probably the Bible. The version of the Bible commonly used today is quite different in style and phrasing from that a mere century ago. It undergoes revisions for clarity and ease of comprehension as linguistic norms change. The subject matter and meaning of its verses have not, however. But have they?    
  
The Bible itself can be seen as a "multi-parallel corpora" (Xia and Yarowsky, 2017, p.448) with multiple versions of what is essentially a highly similar corpus. The Bible used today contains 66 books, each of which is distinct from another in terms of authorship, stylometry and topics. However, the degree to which they differ depends on the books in question. Some, such as the four Gospels of Matthew, Mark, Luke, and John are among the most similar in terms of content and topics, despite being written by different authors. In fact, the Gospels are also known as the Synoptic Gospels (Linmans, 1998; Murai, 2006) with corresponding sections among the four. Murai (2006) argues that from a network analysis perspective, they can be characterized as a series of multiple one-to-many relationships.  
  
Thus, this paper will seek to test the hypothesis that despite controlling for topical similarity, the four Gospel books remain stylometrically distinct from another. This distinction is retained over the ages and various versions of the Bible. This paper will use the King James Version (KJV), American Standard Version (ASV), and World English Bible (WEB) as the basis of comparison.  
  
#### Similarity  
  
First, a spreadsheet of various versions of the Bible was downloaded from BibleHub and the KJV, ASV,and WEB were selected. The preprocessing involved lower-casing and stemming each word, and removing punctuation and numerals. Stopwords were not removed because the English stopwords available through `quanteda` are contemporaneous with English language norms today but not with the times when the KJV and ASV were published. Furthermore, removal of stopwords is likely to result in document-feature matrices (DFMs) with high frequencies of proper nouns in the modern WEB, as well as other stopwords in the older KJV and ASV. This results in an imbalance that might adversely affect a true assessment of their similarities and differences.  
  
Using a bag of words approach, we can calculate the cosine similarity among the four Gospel books. In so far as word choice and frequency contains latent information, it hints at the relative similarity of the four books in terms of topics, and sentiments. 
```{r KJV cosine_similarity, echo=FALSE}
# cosine similarity
cosine_similarity_mat <- sapply(40:43, function(x){
  sapply(40:43, function(y){
    cosine_similarity <- textstat_simil(kjv_books_dfm[x], kjv_books_dfm[y], method = "cosine")
    round(cosine_similarity@x,3)*100
  })
})
row.names(cosine_similarity_mat) <- c("Matthew", "Mark", "Luke", "John")
colnames(cosine_similarity_mat) <- c("Matthew", "Mark", "Luke", "John")
datatable(cosine_similarity_mat, caption = "King James Version")
```

```{r ASV cosine Similarity, echo = FALSE}
# cosine similarity
cosine_similarity_mat <- sapply(1:4, function(x){
  sapply(1:4, function(y){
    cosine_similarity <- textstat_simil(asv_books_dfm[x], asv_books_dfm[y], method = "cosine")
    round(cosine_similarity@x,3)*100
  })
})
row.names(cosine_similarity_mat) <- c("Matthew", "Mark", "Luke", "John")
colnames(cosine_similarity_mat) <- c("Matthew", "Mark", "Luke", "John")
datatable(cosine_similarity_mat, caption = "American Standard Version")
```

```{r WEB Cosine Similarity, echo = FALSE}
# cosine similarity
cosine_similarity_mat <- sapply(1:4, function(x){
  sapply(1:4, function(y){
    cosine_similarity <- textstat_simil(web_books_dfm[x], web_books_dfm[y], method = "cosine")
    round(cosine_similarity@x,3)*100
  })
})
row.names(cosine_similarity_mat) <- c("Matthew", "Mark", "Luke", "John")
colnames(cosine_similarity_mat) <- c("Matthew", "Mark", "Luke", "John")
datatable(cosine_similarity_mat, caption = "World English Bible")
```
  
From the tables above, the average cosine similarity of the four Gospel books across the KJV, ASV,and WEB is 89.1%, 95.4%, 96.1% respectively. This is fairly high, but they are not identically so. The degree of similarity between the books is much higher for the ASV and WEB as compared to the KJV as proven by the mean cosine similarity score. The ASV and WEB agree on the relative similarity of Matthew, Mark, and Luke, but not the KJV. For example, in the KJV, Matthew is most similar to Mark. But in both the ASV and WEB, Mathew is most similar to John. Furthermore, the ASV and WEB suggest that Mark and Luke are more than 98% similar; but in the KJV, it is Matthew which is almost identical to Mark, not Luke. However, the three versions agree that John is most similar to Mark and Luke.    

#### Separability  
  
The second part of this paper builds on the understanding that although the three versions of the Bible and the four Gospels might very well share similar content, they are ultimately distinct. This can be attributed to stylistic differences among the different authors, and translations and word choice contemporaneous to the day and age of each version.  
  
To test this hypothesis, random forest classifiers were fitted on a sample of verses from each of the four books across all three versions. The training and test data split takes into account the slight class imbalance shown below by ensuring that every class is sampled according to its distribution in the population of verses. Three random forest classifiers were fitted:  
1. Classify verses based on the version and the book  
2. Classify verses based on the version  
3. Classify verses based on the book
```{r random forest prep, echo = FALSE}
gospels_class <- all_bibles %>%
  filter(Book %in% c("Matthew", "Mark", "Luke", "John")) %>%
  select(Book, `American Standard Version`, `King James Bible`, `World English Bible`) %>%
  pivot_longer(., cols = c(`American Standard Version`, `King James Bible`, `World English Bible`), names_to = "Version") %>%
  group_by(Book, Version) %>%
  summarise(Count = n()) %>%
  pivot_wider(.,names_from = "Version", values_from = "Count")
datatable(gospels_class, caption = "Class Imbalance: Number of verses in each book and version")
```
  

```{r random forest1, echo = FALSE}
set.seed(1728)

## creating variable class
gospels <- all_bibles %>%
  filter(Book %in% c("Matthew", "Mark", "Luke", "John")) %>%
  select(Book, `American Standard Version`, `King James Bible`, `World English Bible`) %>%
  pivot_longer(., cols = c(`American Standard Version`, `King James Bible`, `World English Bible`), names_to = "Version") %>%
  mutate(Class = paste(substr(Version, 1,1), substr(Book, 1, 3), sep = "_"))

# creating DFM
# partition data with factor to preserve class distribution
gospels_dfm <- dfm(gospels$value, stem = T, remove_punct = T, tolower = T) %>%
  dfm_trim(min_termfreq = 5, min_docfreq = 3) %>%
  convert("matrix")
gospels$Class <- as.factor(gospels$Class)
ids_train <- createDataPartition(gospels$Class, p = 0.7, list = F, times = 1)
train_x1 <- gospels_dfm[ids_train,] %>%
  as.data.frame()
train_y1 <- gospels$Class[ids_train] %>%
  as.factor()
test_x1 <- gospels_dfm[-ids_train,] %>%
  as.data.frame()
test_y1 <- gospels$Class[-ids_train] %>%
  as.factor()

## tuning random forest
mtry <- sqrt(ncol(train_x1)) 
ntree <- 101
trainControl <- trainControl(method = "cv", number = 10, search = "grid" )
metric <- "Accuracy"
tunegrid <- expand.grid(.mtry = mtry)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
 
# running rf
gospels_rf <- train(x = train_x1, y = train_y1,
                  method = "rf", metric = metric, tuneGrid = tunegrid, trControl = trainControl, ntree = ntree, doParallel = T)
stopCluster(cl)

# save RDS and then load to speed things up
saveRDS(gospels_rf, "gospels_rf.RDS")
gospels_rf <- readRDS("gospels_rf.RDS")

# print model
# print(gospels_rf)

# predict and confusion matrix
gospels_rf_predict <- predict(gospels_rf, newdata = test_x1)
gospels_rf_cm <- confusionMatrix(gospels_rf_predict, reference = test_y1, mode = "prec_recall")
gospels_rf_cm$byClass[,c(11,5:7)] %>%
  kable(caption = "Table 1: Model (Book and Version) Results") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) 
```
With 12 classes to predict, the out-of-sample accuracy is `r round(gospels_rf_cm$overall[1],3)`. It is a slight improvement over its base rate of `r round(gospels_rf_cm$overall[5],3)`, which is obtained by simply predicting the most commonly occurring class in the dataset. The confusion matrix (Table A1) suggests that verses from the ASV and KJV are highly likely to be misclassified as the other. Of the four books, it appears that verses from John is the least likely to be misattributed to others whereas verses from Luke and Mark are especially prone to misattribution. This analysis be further broken down into the versions and the books with fewer classes in each case.  
  
```{r random forest 2, echo = FALSE}
set.seed(1728)

## creating variable class
gospels_reduced_class <- all_bibles %>%
  filter(Book %in% c("Matthew", "Mark", "Luke", "John")) %>%
  select(Book, `American Standard Version`, `King James Bible`, `World English Bible`) %>%
  pivot_longer(., cols = c(`American Standard Version`, `King James Bible`, `World English Bible`), names_to = "Version") 

# creating DFM
# partition data with factor to preserve class distribution
gospels_dfm <- dfm(gospels_reduced_class$value, stem = T, remove_punct = T, tolower = T) %>%
  dfm_trim(min_termfreq = 5, min_docfreq = 3) %>%
  convert("matrix")
gospels_reduced_class$Book <- as.factor(gospels_reduced_class$Book)
gospels_reduced_class$Version <- as.factor(gospels_reduced_class$Version)
ids_train2 <- createDataPartition(gospels_reduced_class$Version, p = 0.7, list = F, times = 1)
train_x2 <- gospels_dfm[ids_train2,] %>%
  as.data.frame()
train_y2 <- gospels_reduced_class$Version[ids_train2] %>%
  as.factor()
test_x2 <- gospels_dfm[-ids_train2,] %>%
  as.data.frame()
test_y2 <- gospels_reduced_class$Version[-ids_train2] %>%
  as.factor()

## tuning random forest
mtry <- sqrt(ncol(train_x2)) 
tunegrid <- expand.grid(.mtry = mtry)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

# # running rf
gospels_rf_version <- train(x = train_x2, y = train_y2,
                     method = "rf", metric = metric, tuneGrid = tunegrid, trControl = trainControl, ntree = ntree, doParallel = T)
stopCluster(cl)
 
# save RDS and then load to speed things up
saveRDS(gospels_rf_version, "gospels_rf_version.RDS")
gospels_rf_version <- readRDS("gospels_rf_version.RDS")

# print model
# print(gospels_rf_version)

# predict and confusion matrix
gospels_rf_version_predict <- predict(gospels_rf_version, newdata = test_x2)
gospels_rf_version_cm <- confusionMatrix(gospels_rf_version_predict, reference = test_y2, mode = "prec_recall")
gospels_rf_version_cm$byClass[,c(11, 5:7)] %>%
  kable(caption = "Table 2: Model (Version) Results") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```
  
In the previous model, it was observed that the poor performance of the classifier could be attributed to errors in distinguishing books and versions with seemingly more errors in classification stemming from separating the versions. Training another random forest model to just classify the verses based on its bible version, the out-of-sample accuracy is `r round(gospels_rf_cm$overall[1],3)` is much higher but it is hardly very accurate if taken as a whole. Further, we confirm that the precision and recall rates are especially poor for the ASV and KJV as compared to the WEB.  

```{r random forest 3, echo = FALSE}
##### Book
ids_train3 <- createDataPartition(gospels_reduced_class$Book, p = 0.7, list = F, times = 1)
train_x3 <- gospels_dfm[ids_train3,] %>%
  as.data.frame()
train_y3 <- gospels_reduced_class$Book[ids_train3] %>%
  as.factor()
test_x3 <- gospels_dfm[-ids_train3,] %>%
  as.data.frame()
test_y3 <- gospels_reduced_class$Book[-ids_train3] %>%
  as.factor()

## tuning random forest
mtry <- sqrt(ncol(train_x3)) 
tunegrid <- expand.grid(.mtry = mtry)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

# running rf
gospels_rf_book <- train(x = train_x3, y = train_y3,
                  method = "rf", metric = metric, tuneGrid = tunegrid, trControl = trainControl, ntree = ntree, doParallel = T)
stopCluster(cl)

# save RDS and then load to speed things up
saveRDS(gospels_rf_book, "gospels_rf_book.RDS")
gospels_rf_book <- readRDS("gospels_rf_book.RDS")

# print model
# print(gospels_rf_book)

# predict and confusion matrix
gospels_rf_book_predict <- predict(gospels_rf_book, newdata = test_x3)
gospels_rf_book_cm <- confusionMatrix(gospels_rf_book_predict, reference = test_y3, mode = "prec_recall")
gospels_rf_book_cm$byClass[,c(11,5:7)] %>%
  kable(caption = "Table 3: Model (Book) Results") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```
The out of sample accuracy for predicting the book given a verse is `r round(gospels_rf_book_cm$overall[1], 3)`. Of the three classifiers trained and tested thus far, this shows the highest degree of accuracy. However, given that there is a class imbalance, the F1 score as shown above might be more appropriate instead. The high degree of precision in book classification suggests that distinctions between books are relatively strong and retained over the various versions. However, the book of Mark has a uncharacteristically low recall score of `r round(gospels_rf_book_cm$byClass[3,6], 3)`. This suggests that verses in Mark tend to be misattributed to other books. This lends some support to the theological argument of Marcan Priority in which the Gospels of Matthew and Luke are based on Mark (Goodacre, 2000; Murai, 2006).   

#### Clusters  
    
Murai (2006) argued that the Gospels contain verses that are echoed in one another in syncoptic fashion. Thus, various chapters across the four Gospel books should exhibit high degrees of similarity after controlling for Bible version. This is especially so given the argument of Marcan Priority where Luke and Matthew are derived from the book of Mark.  
  
The WEB was used to test this expectation. The WEB is the most recently translated copy of the three. Removal of contemporary stopwords can thus be performed with relative confidence that textual meanings are not severely redacted. This leaves behind proper nouns and other less commonly used words that will aid in the clustering at the chapter level. Verses can be relatively short and easily taken out of context when stopwords are removed. Collapsing them into chapters can provide a better approximation of chapter meaning.    
```{r PCA, echo = FALSE, fig.height=10, fig.width=12}
web_collapsed_pca <- all_bibles %>%
  filter(Book %in% c("Mark", "John", "Luke", "Matthew")) %>%
  select(Book, Chapter, `World English Bible`) %>%
  group_by(Book, Chapter) %>%
  summarise(Verse = paste(`World English Bible`, collapse = " "))
web_collapsed_pca_vec <- web_collapsed_pca$Verse %>%
  unlist()
names(web_collapsed_pca_vec) <- web_collapsed_pca$Chapter

# create DFM
# convert to matrix
web_chapter_dfm <- dfm(web_collapsed_pca_vec, tolower = T, remove_punct = T, remove_numbers = T, stem = T, remove = stopwords("english"))
web_chapter_mat <- convert(web_chapter_dfm, to = "matrix")
web_chapter_mat_scale <- scale(web_chapter_mat)
# PCA

web_chapter_pca <- prcomp(web_chapter_mat, center = T, scale = T)

web_chapter_pca_df <- web_chapter_pca$x %>%
  as.data.frame()
web_chapter_pca_df$Chapter <- web_collapsed_pca$Chapter
web_chapter_pca_df$Book <- web_collapsed_pca$Book

fviz_eig(web_chapter_pca, addlabels = T,
         main = "Screeplot of WEB DFM's Principal Components",
         caption = "Plot 1: Screeplot of principal components")


# ggplot(web_chapter_pca_df, aes(PC1, PC2, label = Chapter)) + 
#   geom_point(aes(color = Book)) + 
#   #geom_text(size = 3, nudge_y = 0.2, nudge_x = 0.5) + 
#   coord_cartesian(xlim = c(-3, 3), ylim = c(-5, 2.5)) + 
#   labs(title = "First Two Principal Components of WEB DFM",
#        subtitle = "Zoomed to -3:3,-5:2.5 for greater clarity")

# # Distance
chapter_distance <- get_dist(web_chapter_mat_scale)
fviz_dist(chapter_distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07")) + 
  labs(title = "Euclidean Distance of Each Chapter", 
       fill = "Distance",
       caption = "Plot 2: Distance matrix of each chapter")
```
  
As a baseline of comparison, the Euclidean distance between each book can be calculated and plotted. For the most part, books and chapters are distinct from one another with the exception of certain chapters that echo each other content-wise such as Mark 11 and Matthew 21. Curiously, Luke 19 and 20 are dissimilar, possibly because the same content is now split between two chapters.  The screeplot above suggests that dimensionality has not been significantly reduced. The first two dimensions account for less than 7% of the variability. Thus, principal components analysis is not particularly useful in this case as the trade-off between dimensionality and variance is extremely steep.  
  
```{r kmeans, echo=FALSE, fig.height=8, fig.width=10}
# KMeans
set.seed(1728)
fviz_nbclust(web_chapter_mat_scale, kmeans, method = "silhouette", k.max = 28) + 
  theme(axis.title = element_text(size = 8),
        axis.text = element_text(size = 8),
        title = element_text(size = 11)) + 
  labs(title = "Silhouette Score",
       caption = "Plot 3: Optimal Number of Clusters")
k5 <- kmeans(web_chapter_mat_scale, 5, nstart = 25)
fviz_cluster(k5, data = web_chapter_mat_scale, labelsize = 3) + 
  theme(axis.title = element_text(size = 8),
        axis.text = element_text(size = 8),
        title = element_text(size = 11)) + 
  labs(title = "Cluster Plot: Chapters",
       caption = "Plot 4: KMeans Clustering of Chapters")
```
    
KMeans clustering was then performed to see whether books can be identified from one another, in which case there ought to be four clusters. Alternatively, the books can also be clustered by content similarity, in which case there should be anywhere between 16 and 28 clusters given that shortest book has 16 chapters, and the longest has 28. This assumes that content order and chapter delimitation is highly similar. The silhouette score plot suggests that the ideal number of clusters is 5. Plotting the five clusters using the first two principal component, the following observations about the clusters can be made:  
1. Cluster 1 contains chapters on the crucifixion of Jesus.  
2. Cluster 4 contains chapters on the betrayal of Jesus by Judas and the Last Supper.  
3. Cluster 3 and 4 are outliers.  
4. Cluster 2 is most probably an amalgamation of dissimilar chapters, given that most chapters are more different than they are similar, as the Euclidean distance plot suggests.  
  
#### Appendix
```{r Appendix, echo=FALSE}
# rf 1
gospels_rf_cm$table %>%
  kable(caption = "Table A1: Predicting Book and Version with Random Forests | Prediction(V), Reference(H)") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  pack_rows("ASV", 1,4) %>%
  pack_rows("KJV", 5,8) %>%
  pack_rows("WEB",9,12)

# rf 2
gospels_rf_version_cm$table %>%
  kable(caption = "Table A2: Predicting Version with Random Forests | Prediction(V), Reference(H)") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

# rf 3
gospels_rf_book_cm$table %>%
  kable(caption = "Table A3: Predicting Book with Random Forests | Prediction(V), Reference(H)") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))  
```

  

